# Project Specification: Sequential CSV to Kafka Processor

## Project Overview

**Project Name:** AWS Sequential CSV to Kafka Processor  
**Version:** 1.0.0  
**Created:** January 16, 2026  
**Purpose:** Process 100 million rows from S3 CSV file to AWS Kafka (MSK) with guaranteed sequential ordering and zero data loss

## Business Requirements

### Primary Objective
Transfer data from a large CSV file (100M rows) stored in Amazon S3 to an Amazon MSK (Managed Kafka) topic while maintaining:
1. Exact sequential order of records
2. Zero data loss
3. No duplicate records
4. Ability to recover from failures

### Constraints
- **Sequentiality:** Records must be processed in exact CSV order (row 1, then row 2, etc.)
- **No Parallelization:** Cannot split CSV into chunks as this breaks ordering
- **AWS Ecosystem:** All components must be AWS-native services
- **Data Integrity:** Must guarantee exactly 100M records in Kafka if CSV has 100M rows

## Technical Requirements

### Functional Requirements

1. **Data Source**
   - Read CSV file from Amazon S3
   - Support files with 100M+ rows
   - Stream data (no full file download to memory)
   - Handle large file sizes (multi-GB)

2. **Data Destination**
   - Write to Amazon MSK (Managed Kafka) topic
   - Use single partition for ordering guarantee
   - Confirm each write before proceeding

3. **Ordering Guarantee**
   - Process records sequentially (one at a time)
   - Maintain CSV row order in Kafka
   - Use single Kafka partition

4. **Reliability**
   - Checkpoint progress every N records
   - Resume from last checkpoint on failure
   - Retry failed Kafka writes
   - No silent data loss

5. **Recovery**
   - Store checkpoint in DynamoDB
   - Automatic resume from last successful record
   - Handle process crashes, network failures, Spot interruptions

### Non-Functional Requirements

1. **Performance**
   - Target: 10,000-50,000 records/second
   - Complete 100M rows in 30 minutes to 3 hours
   - Minimize memory usage (streaming)

2. **Cost**
   - Provide multiple cost options (serverless to Spot)
   - Optimize for AWS cost structure
   - Use Spot instances where possible

3. **Monitoring**
   - CloudWatch logs for all operations
   - DynamoDB progress tracking
   - Error logging and alerting

4. **Maintainability**
   - Infrastructure as Code (Terraform)
   - Clear documentation
   - Modular design

## Solution Architecture

### Three Implementation Options

#### Option 1: ECS Fargate
- **Use Case:** Production workloads
- **Compute:** AWS Fargate (serverless containers)
- **Cost:** $0.50-2.00 per run
- **Management:** Fully managed
- **Monitoring:** Excellent (CloudWatch integration)

#### Option 2: AWS Batch with Spot
- **Use Case:** Cost-optimized batch processing
- **Compute:** EC2 Spot instances via AWS Batch
- **Cost:** $0.10-0.50 per run
- **Management:** Fully managed with auto-retry
- **Monitoring:** Good (CloudWatch integration)

#### Option 3: EC2 Spot Instance
- **Use Case:** One-off jobs, maximum cost savings
- **Compute:** Single EC2 Spot instance
- **Cost:** $0.01-0.05 per run
- **Management:** Manual
- **Monitoring:** Basic (instance logs)

### Common Components (All Options)

1. **Amazon S3**
   - Stores source CSV file
   - Streamed line-by-line (not downloaded)

2. **Amazon MSK (Managed Kafka)**
   - Target Kafka cluster
   - Single partition for ordering
   - Replication factor ≥ 2 for durability

3. **Amazon DynamoDB**
   - Checkpoint table
   - Stores: job_id, last_line, total_sent, timestamp
   - On-demand billing mode

4. **Amazon CloudWatch**
   - Logs for all processing
   - Monitoring and alerting

5. **AWS IAM**
   - Roles and policies for S3, MSK, DynamoDB access
   - Least privilege principle

### Data Flow

```
S3 CSV File
    ↓ (stream read)
Processor (ECS/Batch/EC2)
    ↓ (sequential send)
Kafka Topic (Partition 0)
    ↓ (checkpoint every 10K)
DynamoDB (progress tracking)
```

## Technical Design

### Kafka Producer Configuration

```python
KafkaProducer(
    bootstrap_servers=<MSK_BROKERS>,
    value_serializer=json.dumps,
    max_in_flight_requests_per_connection=1,  # Ordering
    acks='all',                                # Durability
    retries=2147483647,                        # Infinite retry
    enable_idempotence=True,                   # No duplicates
    compression_type='gzip'                    # Efficiency
)
```

### Checkpoint Strategy

- **Frequency:** Every 10,000 records
- **Storage:** DynamoDB table
- **Data:** `{job_id, last_line, total_sent, timestamp, status}`
- **Recovery:** Read checkpoint on start, skip to last_line + 1

### Error Handling

1. **Kafka Write Failure**
   - Retry up to 5 times with exponential backoff
   - If all retries fail: stop processing, save checkpoint
   - Requires manual intervention

2. **Network Failure**
   - Kafka producer retries automatically
   - Exponential backoff
   - Stop if timeout exceeded

3. **Process Crash**
   - Checkpoint saved every 10K records
   - Restart reads checkpoint
   - Resume from last confirmed record

4. **Spot Interruption**
   - 2-minute warning (not always graceful)
   - Last checkpoint in DynamoDB
   - Restart automatically (Batch) or manually (EC2)

### Record Enrichment

Each record is enriched with metadata:
```python
{
    ...original_csv_fields,
    '_source_line': <line_number>,
    '_source_file': 's3://bucket/key'
}
```

## Infrastructure Components

### Terraform Modules

Each option includes:
- `main.tf` - Core infrastructure
- `variables.tf` - Configuration parameters
- `outputs.tf` - Resource identifiers
- `terraform.tfvars.example` - Configuration template

### Required AWS Resources

1. **IAM Roles**
   - Task execution role (ECS/Batch)
   - Task role (application permissions)
   - Instance role (EC2)

2. **Security Groups**
   - Egress to internet (for S3, MSK, DynamoDB)
   - No ingress required

3. **VPC Configuration**
   - Private subnets with NAT gateway
   - Route to internet for AWS services

4. **ECR Repository** (Options 1 & 2)
   - Docker image storage
   - Automatic scanning enabled

## Deployment Process

### Option 1: ECS Fargate

1. Configure `terraform.tfvars`
2. Run `terraform apply`
3. Build Docker image
4. Push to ECR
5. Run ECS task

### Option 2: AWS Batch

1. Configure `terraform.tfvars`
2. Run `terraform apply`
3. Build Docker image
4. Push to ECR
5. Submit Batch job

### Option 3: EC2 Spot

1. Configure `user-data.sh` or `terraform.tfvars`
2. Run `terraform apply` OR launch instance manually
3. Instance self-terminates on completion

## Monitoring and Verification

### Progress Monitoring

```bash
# Check DynamoDB checkpoint
aws dynamodb get-item \
  --table-name csv-kafka-checkpoint \
  --key '{"job_id":{"S":"csv-job-1"}}'

# Check CloudWatch logs
aws logs tail /ecs/csv-to-kafka --follow
```

### Verification

```bash
# Verify record count in Kafka
kafka-run-class kafka.tools.GetOffsetShell \
  --broker-list $KAFKA_BROKERS \
  --topic $TOPIC \
  --time -1
```

## File Structure

```
.
├── README.md                          # Main documentation
├── COMPARISON.md                      # Solution comparison
├── GUARANTEES.md                      # Data delivery guarantees
├── PROJECT_SPEC.md                    # This file
├── DOCUMENTATION.md                   # Code documentation
├── .gitignore                         # Git ignore rules
│
├── option1-ecs-fargate/
│   ├── README.md                      # ECS-specific guide
│   ├── deploy.sh                      # Deployment script
│   ├── app/
│   │   ├── Dockerfile                 # Container definition
│   │   ├── processor.py               # Main application
│   │   └── requirements.txt           # Python dependencies
│   └── terraform/
│       ├── main.tf                    # Infrastructure
│       ├── variables.tf               # Configuration
│       ├── outputs.tf                 # Resource outputs
│       └── terraform.tfvars.example   # Config template
│
├── option2-aws-batch/
│   ├── README.md                      # Batch-specific guide
│   ├── deploy.sh                      # Deployment script
│   ├── app/
│   │   ├── Dockerfile                 # Container definition
│   │   ├── processor.py               # Main application
│   │   └── requirements.txt           # Python dependencies
│   └── terraform/
│       ├── main.tf                    # Infrastructure
│       ├── variables.tf               # Configuration
│       ├── outputs.tf                 # Resource outputs
│       └── terraform.tfvars.example   # Config template
│
└── option3-ec2-spot/
    ├── README.md                      # EC2-specific guide
    ├── user-data.sh                   # Bootstrap script
    ├── launch-spot.sh                 # Manual launch script
    └── terraform/
        ├── main.tf                    # Infrastructure
        ├── variables.tf               # Configuration
        ├── outputs.tf                 # Resource outputs
        └── terraform.tfvars.example   # Config template
```

## Configuration Parameters

### Required Parameters (All Options)

| Parameter | Description | Example |
|-----------|-------------|---------|
| `aws_region` | AWS region | `us-east-1` |
| `s3_bucket` | S3 bucket name | `my-data-bucket` |
| `s3_key` | CSV file path | `data/export.csv` |
| `kafka_bootstrap_servers` | MSK brokers | `b-1.msk.amazonaws.com:9092` |
| `kafka_topic` | Target topic | `csv-import` |
| `msk_cluster_arn` | MSK cluster ARN | `arn:aws:kafka:...` |
| `vpc_id` | VPC ID | `vpc-xxxxx` |
| `subnet_ids` | Subnet IDs | `["subnet-xxxxx"]` |
| `job_id` | Unique job ID | `csv-job-1` |

### Optional Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| `task_cpu` | CPU units (ECS) | `2048` |
| `task_memory` | Memory MB (ECS) | `4096` |
| `instance_type` | EC2 type | `t3.medium` |

## Testing Strategy

### Unit Testing
- Test checkpoint manager
- Test Kafka producer configuration
- Test error handling logic

### Integration Testing
- Test with small CSV (1000 rows)
- Verify all records in Kafka
- Test checkpoint recovery
- Test failure scenarios

### Load Testing
- Test with 1M row CSV
- Measure throughput
- Monitor memory usage
- Verify performance targets

## Success Criteria

1. ✅ All 100M records transferred to Kafka
2. ✅ Records in exact CSV order
3. ✅ No duplicate records
4. ✅ Recovery works after failure
5. ✅ Checkpoint accuracy verified
6. ✅ Performance within targets (30min-3hrs)
7. ✅ Cost within estimates
8. ✅ Monitoring and logging functional

## Risks and Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| Spot interruption | Processing stops | Checkpointing + auto-retry (Batch) |
| Kafka cluster failure | Processing stops | Infinite retries + manual intervention |
| Network issues | Slow/failed processing | Exponential backoff + retries |
| Out of memory | Process crash | Streaming read (no full file load) |
| Cost overrun | Budget exceeded | Use Spot instances, monitor costs |

## Future Enhancements

1. **Parallel Processing**
   - Split CSV by ranges with ordering within ranges
   - Multiple partitions with partition keys

2. **Schema Validation**
   - Validate CSV schema before processing
   - Reject invalid records

3. **Dead Letter Queue**
   - Send failed records to DLQ
   - Continue processing instead of stopping

4. **Metrics Dashboard**
   - Real-time throughput monitoring
   - Progress visualization
   - Cost tracking

5. **Multi-file Support**
   - Process multiple CSV files
   - Maintain ordering across files

## Support and Maintenance

### Troubleshooting

See individual README files for solution-specific troubleshooting.

Common issues:
- IAM permission errors
- Network connectivity to MSK
- DynamoDB throttling
- Kafka broker unavailable

### Maintenance Tasks

- Monitor CloudWatch logs
- Check DynamoDB for stuck jobs
- Review Kafka topic lag
- Update dependencies periodically

## References

- [Amazon MSK Documentation](https://docs.aws.amazon.com/msk/)
- [Kafka Producer Configuration](https://kafka.apache.org/documentation/#producerconfigs)
- [AWS Batch Documentation](https://docs.aws.amazon.com/batch/)
- [ECS Fargate Documentation](https://docs.aws.amazon.com/ecs/)
- [smart_open Library](https://github.com/RaRe-Technologies/smart_open)

## Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.0.0 | 2026-01-16 | Initial release with 3 options |

## License

This project is provided as-is for internal use.
